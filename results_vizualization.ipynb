{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7159e3b-518d-4e56-86af-7c8a25c7adcd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import pandas as pd\n",
    "import os\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87e4af85",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_judge_paths = glob(\"./data/judgements/judge_*/*/*.json\")\n",
    "metric_judge_paths = glob(\"./data/judgements/metrics/*/*.csv\")\n",
    "model_result_paths = llm_judge_paths + metric_judge_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e02ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_dataset_dict = {\n",
    "    \"elyza__ELYZA-tasks-100\": \"ELYZA-Tasks\",\n",
    "    \"yuzuai__rakuda-questions\": \"Rakuda\",\n",
    "    \"lightblue__tengu_bench\": \"Tengu-Bench\",\n",
    "    \"shisa-ai__ja-mt-bench-1shot\": \"MT-Bench\",\n",
    "    \"lmg-anon__VNTL-v3.1-1k\": \"VNTL-Translation\"\n",
    "}\n",
    "\n",
    "eval_dataset_map = {name: None for name in eval_dataset_dict.values()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cae7633",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_eval_dataset_dict():\n",
    "    base_path = './data/model_answers'\n",
    "    dataset_dirs = [d for d in os.listdir(base_path) if os.path.isdir(os.path.join(base_path, d))]\n",
    "    dataset_dict = {}\n",
    "    for dataset in dataset_dirs:\n",
    "        dataset_path = os.path.join(base_path, dataset)\n",
    "        first_file = next((f for f in os.listdir(dataset_path) if os.path.isfile(os.path.join(dataset_path, f))), None)\n",
    "        if first_file:\n",
    "            fpath = os.path.join(dataset_path, first_file)\n",
    "            with open(fpath, encoding='utf-8') as f:\n",
    "                total_lines = sum(1 for _ in f)\n",
    "            label = eval_dataset_dict.get(dataset, dataset)\n",
    "            \n",
    "            dataset_dict[dataset] = f\"{label}-{total_lines}\"\n",
    "            eval_dataset_map[label] = f\"{label}-{total_lines}\"\n",
    "        else:\n",
    "            dataset_dict[dataset] = dataset\n",
    "    return dataset_dict\n",
    "\n",
    "eval_dataset_dict = get_eval_dataset_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9221fdf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_datasets = list(eval_dataset_dict.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe36503f-515f-4e29-b58f-81b1684734b0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "all_result_dfs = []\n",
    "\n",
    "for model_result_path in model_result_paths:\n",
    "    path_parts = model_result_path.replace('/', os.sep).split(os.sep)\n",
    "    \n",
    "    try:\n",
    "        if model_result_path.endswith('.json'):\n",
    "            temp_df = pd.read_json(model_result_path, lines=True)\n",
    "            temp_df[\"judge_model\"] = path_parts[-3]\n",
    "            temp_df[\"eval_dataset\"] = eval_dataset_dict[path_parts[-2]]\n",
    "            temp_df[\"model_name\"] = path_parts[-1].replace(\".json\", \"\")\n",
    "        \n",
    "        elif model_result_path.endswith('.csv'):\n",
    "            temp_df = pd.read_csv(model_result_path)\n",
    "            # Use BLEU score and scale it to be 0-10 for averaging\n",
    "            temp_df['score'] = temp_df['bleu'] / 10.0\n",
    "            temp_df[\"judge_model\"] = 'Metrics (BLEU/chrF)'\n",
    "            temp_df[\"eval_dataset\"] = eval_dataset_dict[path_parts[-2]]\n",
    "            temp_df[\"model_name\"] = path_parts[-1].replace(\".csv\", \"\")\n",
    "\n",
    "        all_result_dfs.append(temp_df)\n",
    "    except Exception as e:\n",
    "        print(f\"Skipping file due to error: {model_result_path} -> {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d63791e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consolidate and clean the final DataFrame\n",
    "all_result_df = pd.concat(all_result_dfs, ignore_index=True)\n",
    "all_result_df['score'] = pd.to_numeric(all_result_df['score'], errors='coerce')\n",
    "all_result_df.dropna(subset=['score'], inplace=True)\n",
    "all_result_df.to_csv(\"output.csv\", index=False)\n",
    "print(\"Combined results saved to output.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ae39c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale ELYZA scores to be on a 10-point scale instead of 5\n",
    "# The .loc accessor is used to select the correct rows and multiply the 'score'\n",
    "\n",
    "if 'ELYZA-Tasks' in eval_dataset_map:\n",
    "    ELYZA_NAME = eval_dataset_map[\"ELYZA-Tasks\"]\n",
    "    print(f\"Max score for ELYZA before scaling: {all_result_df.loc[all_result_df['eval_dataset'] == ELYZA_NAME, 'score'].max():.2f}\")\n",
    "    # Scale ELYZA scores to be on a 10-point scale\n",
    "    all_result_df.loc[all_result_df['eval_dataset'] == ELYZA_NAME, 'score'] *= 2\n",
    "    print(f\"Max score for ELYZA after scaling: {all_result_df.loc[all_result_df['eval_dataset'] == ELYZA_NAME, 'score'].max():.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7afc4168",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Create a detailed summary pivot table ---\n",
    "def get_bench_label(bench):\n",
    "    if 'elyza' in bench.lower(): return 'ELYZA 100'\n",
    "    if 'mt-bench' in bench.lower(): return 'JA-MT'\n",
    "    if 'rakuda' in bench.lower(): return 'Rakuda'\n",
    "    if 'tengu' in bench.lower(): return 'Tengu'\n",
    "    if 'vntl' in bench.lower(): return 'VNTL' # Added for new benchmark\n",
    "    return bench\n",
    "\n",
    "all_result_df['bench_label'] = all_result_df['eval_dataset'].apply(get_bench_label)\n",
    "all_result_df['bench_judge_label'] = all_result_df['bench_label'] + ' (' + all_result_df['judge_model'] + ')'\n",
    "\n",
    "pivot = all_result_df.pivot_table(\n",
    "    index='model_name',\n",
    "    columns='bench_judge_label',\n",
    "    values='score',\n",
    "    aggfunc='mean'\n",
    ")\n",
    "pivot['Average (All)'] = pivot.mean(axis=1)\n",
    "col_order = sorted([col for col in pivot.columns if col != 'Average (All)']) + ['Average (All)']\n",
    "pivot = pivot[col_order].sort_values(by='Average (All)', ascending=False)\n",
    "\n",
    "float_cols = pivot.select_dtypes(include=['float', 'float64']).columns\n",
    "for col in float_cols:\n",
    "    pivot[col] = pivot[col].apply(lambda x: format(x, '.2f') if pd.notnull(x) else '')\n",
    "\n",
    "display(pivot)\n",
    "pivot.to_csv('summary_output.csv', index=True, quoting=1)\n",
    "print(\"Detailed pivot table summary saved to summary_output.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a989b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Create a styled correlation-style table ---\n",
    "eval_dataset_names = all_result_df.eval_dataset.unique()\n",
    "model_names = all_result_df.model_name.unique()\n",
    "\n",
    "eval_corr_results = {model_name: {} for model_name in model_names}\n",
    "for eval_dataset_name in eval_dataset_names:\n",
    "    for model_name in model_names:\n",
    "        score = all_result_df[(all_result_df.eval_dataset == eval_dataset_name) & (all_result_df.model_name == model_name)].score.mean()\n",
    "        eval_corr_results[model_name][eval_dataset_name] = score\n",
    "\n",
    "eval_res_df = pd.DataFrame(eval_corr_results).T # Transpose to have models as rows\n",
    "\n",
    "eval_res_df['mean'] = eval_res_df.mean(axis=1)\n",
    "eval_res_df = eval_res_df.sort_values(by='mean', ascending=False)\n",
    "\n",
    "def highlight_max(s):\n",
    "    is_max = s.max()\n",
    "    return ['background-color: #FFF8C4' if v == is_max else '' for v in s]\n",
    "\n",
    "styled_df = eval_res_df.style.apply(highlight_max, axis=0).format(\"{:.2f}\").set_caption(\"Model Mean Scores by Benchmark\")\n",
    "display(styled_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b66da917",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "\n",
    "def get_model_size(model_name_str):\n",
    "    try:\n",
    "        size_match = re.search(r\"\\b(\\d{1,3})[bB]\\b\", model_name_str)\n",
    "        if size_match: return int(size_match.group(1))\n",
    "        size_match = re.search(r\"-(\\d{1,3})b\", model_name_str, re.IGNORECASE)\n",
    "        if size_match: return int(size_match.group(1))\n",
    "        return None\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "model_size_df = eval_res_df.copy()\n",
    "model_size_df['model_size'] = model_size_df.index.to_series().apply(get_model_size)\n",
    "size_df = model_size_df.dropna(subset=['model_size']).groupby('model_size').mean()\n",
    "size_df['model_size_log'] = np.log(size_df.index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9cc91b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plot_columns = [col for col in eval_datasets if col in size_df.columns]\n",
    "for column in plot_columns:\n",
    "    sns.regplot(x='model_size_log', y=column, data=size_df, scatter=True, label=column, ci=None)\n",
    "\n",
    "plt.xticks(size_df['model_size_log'], size_df.index.astype(int).astype(str) + \"B\")\n",
    "plt.legend()\n",
    "plt.title('Model Size (Log Scale) vs. Scores with Regression Lines')\n",
    "plt.xlabel(\"Model Size\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.grid(True, which='both', linestyle='--')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ebe802",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "\n",
    "# --- Data Preparation ---\n",
    "primary_judge = 'judge_gpt-4.1'\n",
    "df_filtered = all_result_df[all_result_df['judge_model'].isin([primary_judge, 'Metrics (BLEU/chrF)'])]\n",
    "mean_df = df_filtered.groupby([\"model_name\", \"eval_dataset\"])['score'].mean().reset_index()\n",
    "model_order = mean_df.groupby('model_name')['score'].mean().sort_values(ascending=False).index.tolist()\n",
    "unique_benchmarks = sorted(mean_df['eval_dataset'].unique())\n",
    "color_map = {model: px.colors.qualitative.Plotly[i % len(px.colors.qualitative.Plotly)] for i, model in enumerate(model_order)}\n",
    "\n",
    "def hex_to_rgba(h, a):\n",
    "    h = h.lstrip('#')\n",
    "    return f'rgba({int(h[0:2], 16)}, {int(h[2:4], 16)}, {int(h[4:6], 16)}, {a})'\n",
    "\n",
    "# --- Create Radar Chart ---\n",
    "fig_radar = go.Figure()\n",
    "for model_name in model_order:\n",
    "    model_subset = mean_df[mean_df['model_name'] == model_name].set_index('eval_dataset').reindex(unique_benchmarks).reset_index()\n",
    "    fig_radar.add_trace(go.Scatterpolar(\n",
    "        r=model_subset[\"score\"],\n",
    "        theta=model_subset[\"eval_dataset\"],\n",
    "        fill='toself',\n",
    "        name=model_name,\n",
    "        mode='lines+markers',\n",
    "        line=dict(color=color_map[model_name]),\n",
    "        fillcolor=hex_to_rgba(color_map[model_name], 0.2),\n",
    "        hovertemplate=\"<b>%{fullData.name}</b><br>Score: %{r:.2f}<extra></extra>\"\n",
    "    ))\n",
    "\n",
    "# --- Update Layout with Final Spacing ---\n",
    "fig_radar.update_layout(\n",
    "    title=dict(\n",
    "        text=f\"Model Performance Radar Chart<br><sup>Judge: {primary_judge.replace('judge_', '')}</sup>\",\n",
    "        font=dict(size=20),\n",
    "        x=0.5,\n",
    "        y=0.95 # Keep title positioned near the top of the figure\n",
    "    ),\n",
    "    polar=dict(\n",
    "        radialaxis=dict(visible=True, range=[0, 10])\n",
    "    ),\n",
    "    legend=dict(\n",
    "        orientation='h',\n",
    "        yanchor='bottom',\n",
    "        y=-0.2, # Push legend down to create space\n",
    "        xanchor='center',\n",
    "        x=0.5\n",
    "    ),\n",
    "    template='plotly_white',\n",
    "    margin=dict(\n",
    "        l=50,  # Left margin\n",
    "        r=50,  # Right margin\n",
    "        b=100, # Bottom margin (for legend)\n",
    "        t=100, # Top margin (for title)\n",
    "        pad=4  # Padding between plot and margin\n",
    "    )\n",
    ")\n",
    "\n",
    "fig_radar.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82216dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# --- Horizontal Bar Chart Grid (Looping through all judges) ---\n",
    "for judge_model in all_result_df['judge_model'].unique():\n",
    "    df_judge = all_result_df[all_result_df['judge_model'] == judge_model]\n",
    "    if df_judge.empty: continue\n",
    "\n",
    "    mean_df_judge = df_judge.groupby([\"model_name\", \"eval_dataset\"]).score.mean().reset_index()\n",
    "    model_order = mean_df_judge.groupby('model_name')['score'].mean().sort_values(ascending=False).index.tolist()\n",
    "    color_map = {model: px.colors.qualitative.Plotly[i % len(px.colors.qualitative.Plotly)] for i, model in enumerate(model_order)}\n",
    "    benchmarks = sorted(mean_df_judge['eval_dataset'].unique())\n",
    "    avg_scores = mean_df_judge.groupby('model_name')['score'].mean().reindex(model_order)\n",
    "    \n",
    "    n_rows, n_cols = 3, 2\n",
    "    subplot_titles = benchmarks[:5]\n",
    "    subplot_titles.append(\"Average\")\n",
    "    while len(subplot_titles) < 6: subplot_titles.append(\"\")\n",
    "\n",
    "    fig_horizontal = make_subplots(rows=n_rows, cols=n_cols, subplot_titles=subplot_titles, vertical_spacing=0.1, horizontal_spacing=0.05)\n",
    "\n",
    "    for i, title in enumerate(subplot_titles):\n",
    "        if not title: continue\n",
    "        row, col = i // n_cols + 1, i % n_cols + 1\n",
    "        \n",
    "        plot_data = avg_scores if title == \"Average\" else mean_df_judge[mean_df_judge['eval_dataset'] == title].set_index('model_name')['score'].reindex(model_order)\n",
    "        \n",
    "        for model_name in model_order:\n",
    "            score = plot_data.get(model_name, float('nan'))\n",
    "            fig_horizontal.add_trace(go.Bar(\n",
    "                y=[model_name], x=[score], name=model_name, marker_color=color_map[model_name],\n",
    "                text=[f\"{score:.2f}\" if pd.notnull(score) else \"\"], textposition='outside', orientation='h',\n",
    "                showlegend=(i == 0)\n",
    "            ), row=row, col=col)\n",
    "        \n",
    "        fig_horizontal.update_xaxes(range=[0, 10.5], row=row, col=col)\n",
    "        fig_horizontal.update_yaxes(showticklabels=False, autorange='reversed', row=row, col=col)\n",
    "\n",
    "    fig_horizontal.update_layout(\n",
    "        height=1000, width=1200,\n",
    "        title_text=f\"Model Score Comparison - Horizontal (Judge: {judge_model.replace('judge_', '')})\",\n",
    "        title_x=0.5, template='plotly_white', legend_title_text='Model',\n",
    "        bargap=0.01, bargroupgap=0.01\n",
    "    )\n",
    "    fig_horizontal.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98b3c2ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# VERTICAL BAR CHART GRID (LOOPING)\n",
    "for judge_model in all_result_df['judge_model'].unique():\n",
    "    df_judge = all_result_df[all_result_df['judge_model'] == judge_model]\n",
    "    if df_judge.empty: continue\n",
    "\n",
    "    mean_df_judge = df_judge.groupby([\"model_name\", \"eval_dataset\"]).score.mean().reset_index()\n",
    "    model_order = mean_df_judge.groupby('model_name')['score'].mean().sort_values(ascending=False).index.tolist()\n",
    "    color_map = {model: px.colors.qualitative.Plotly[i % len(px.colors.qualitative.Plotly)] for i, model in enumerate(model_order)}\n",
    "    benchmarks = sorted(mean_df_judge['eval_dataset'].unique())\n",
    "    avg_scores = mean_df_judge.groupby('model_name')['score'].mean().reindex(model_order)\n",
    "    \n",
    "    n_rows, n_cols = 3, 2\n",
    "    subplot_titles = benchmarks[:5]\n",
    "    subplot_titles.append(\"Average\")\n",
    "    while len(subplot_titles) < 6: subplot_titles.append(\"\")\n",
    "\n",
    "    fig_vertical = make_subplots(rows=n_rows, cols=n_cols, subplot_titles=subplot_titles, vertical_spacing=0.1, horizontal_spacing=0.05)\n",
    "    \n",
    "    for i, title in enumerate(subplot_titles):\n",
    "        if not title: continue\n",
    "        row, col = i // n_cols + 1, i % n_cols + 1\n",
    "        \n",
    "        plot_data = avg_scores if title == \"Average\" else mean_df_judge[mean_df_judge['eval_dataset'] == title].set_index('model_name')['score'].reindex(model_order)\n",
    "        \n",
    "        for model_name in model_order:\n",
    "            score = plot_data.get(model_name, float('nan'))\n",
    "            fig_vertical.add_trace(go.Bar(\n",
    "                x=[model_name], y=[score], name=model_name, marker_color=color_map[model_name],\n",
    "                text=[f\"{score:.2f}\" if pd.notnull(score) else \"\"], textposition='outside',\n",
    "                showlegend=(i == 0)\n",
    "            ), row=row, col=col)\n",
    "            \n",
    "        fig_vertical.update_yaxes(range=[0, 10.5], row=row, col=col)\n",
    "        fig_vertical.update_xaxes(showticklabels=False, row=row, col=col)\n",
    "\n",
    "    fig_vertical.update_layout(\n",
    "        height=1000, width=1200, barmode='group',\n",
    "        title_text=f\"Model Score Comparison - Vertical (Judge: {judge_model.replace('judge_', '')})\",\n",
    "        title_x=0.5, template='plotly_white',\n",
    "        legend_title_text='Model',\n",
    "        bargap=0.01, bargroupgap=0.01\n",
    "    )\n",
    "    fig_vertical.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1839f7f1",
   "metadata": {},
   "source": [
    "# Benchmark Descriptions\n",
    "\n",
    "* **ELYZA-tasks-100**: A Japanese benchmark consisting of 100 diverse tasks, designed to evaluate general language understanding and generation capabilities of LLMs in Japanese. Tasks include question answering, summarization, and also translation evaluation.\n",
    "\n",
    "* **Rakuda**: A Japanese benchmark focused on evaluating LLMs' performance on a wide range of question-answering tasks, including both factual and reasoning-based questions.\n",
    "\n",
    "* **Tengu-Bench**: A comprehensive Japanese benchmark that tests LLMs on various categories such as knowledge, reasoning, and reading comprehension, aiming to provide a broad assessment of model capabilities.\n",
    "\n",
    "* **MT-Bench**: The Japanese adaptation of the MT-Bench, which is a multi-turn dialogue benchmark. It evaluates LLMs' ability to handle conversational tasks, including context retention, instruction following, and multi-turn reasoning.\n",
    "\n",
    "* **VNTL-Translation**: A benchmark specifically designed for evaluating the quality of Japanese-to-English translation. It consists of text from Japanese visual novels, testing the model's ability to handle narrative and colloquial language. Performance is measured using standard translation metrics like BLEU and chrF."
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cpu.m118",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/base-cpu:m118"
  },
  "kernelspec": {
   "display_name": "eval",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
